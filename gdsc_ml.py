# -*- coding: utf-8 -*-
"""GDSC_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d688RAuj04bnI3nuX0kwzqKA9RRQDxGj
"""

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
from sklearn.metrics import classification_report

# 1. Load and Preprocess the CIFAR-10 Dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Normalize images to [0, 1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Convert labels to one-hot encoding
num_classes = 10
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)
y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)

# 2. Build the CNN Model
model = models.Sequential([
    # First Convolution Block
    layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
    layers.BatchNormalization(),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.25),

    # Second Convolution Block
    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.25),

    # Fully Connected Layers
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

# Compile the model
model.compile(
    loss='categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Hyperparameter: learning rate
    metrics=['accuracy']
)

# 3. Configure Early Stopping
early_stop = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# 4. Data Augmentation (improves generalization)
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True
)
datagen.fit(x_train)

# 5. Train the Model
batch_size = 64  # Hyperparameter: batch size
epochs = 10     # You may adjust the maximum number of epochs

history = model.fit(
    datagen.flow(x_train, y_train_cat, batch_size=batch_size),
    steps_per_epoch=x_train.shape[0] // batch_size,
    epochs=epochs,
    validation_data=(x_test, y_test_cat),
    callbacks=[early_stop]
)

# 6. Evaluate the Model on Test Data
test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)
print("Test Accuracy:", test_acc)

# Make predictions and compute detailed classification metrics
y_pred = np.argmax(model.predict(x_test), axis=1)
y_true = y_test.flatten()

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=[str(i) for i in range(num_classes)]))


# =======================================================
# Optional Bonus: Transfer Learning using VGG16
# =======================================================
# Note: VGG16 normally expects larger images. Since CIFAR-10 images are 32x32,
# this example uses VGG16 with include_top=False. In practice, you might consider
# resizing images for better performance with transfer learning.
from tensorflow.keras.applications import VGG16

# Load the pre-trained VGG16 model without the top layers
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(32, 32, 3)
)
base_model.trainable = False  # Freeze the base model

# Build the transfer learning model
model_tl = models.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

# Compile the transfer learning model
model_tl.compile(
    loss='categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# Train the transfer learning model
history_tl = model_tl.fit(
    datagen.flow(x_train, y_train_cat, batch_size=batch_size),
    steps_per_epoch=x_train.shape[0] // batch_size,
    epochs=epochs,
    validation_data=(x_test, y_test_cat),
    callbacks=[early_stop]
)

# Evaluate the transfer learning model
test_loss_tl, test_acc_tl = model_tl.evaluate(x_test, y_test_cat, verbose=0)
print("\nTransfer Learning Test Accuracy:", test_acc_tl)